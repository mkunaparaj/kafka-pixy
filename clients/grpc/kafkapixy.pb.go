// Code generated by protoc-gen-go. DO NOT EDIT.
// source: kafkapixy.proto

package pb

import (
	context "context"
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

type RecordHeader struct {
	// Key in the header key-value pair
	Key string `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
	// Value in the header key-value pair
	Value                []byte   `protobuf:"bytes,2,opt,name=value,proto3" json:"value,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RecordHeader) Reset()         { *m = RecordHeader{} }
func (m *RecordHeader) String() string { return proto.CompactTextString(m) }
func (*RecordHeader) ProtoMessage()    {}
func (*RecordHeader) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{0}
}

func (m *RecordHeader) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RecordHeader.Unmarshal(m, b)
}
func (m *RecordHeader) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RecordHeader.Marshal(b, m, deterministic)
}
func (m *RecordHeader) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RecordHeader.Merge(m, src)
}
func (m *RecordHeader) XXX_Size() int {
	return xxx_messageInfo_RecordHeader.Size(m)
}
func (m *RecordHeader) XXX_DiscardUnknown() {
	xxx_messageInfo_RecordHeader.DiscardUnknown(m)
}

var xxx_messageInfo_RecordHeader proto.InternalMessageInfo

func (m *RecordHeader) GetKey() string {
	if m != nil {
		return m.Key
	}
	return ""
}

func (m *RecordHeader) GetValue() []byte {
	if m != nil {
		return m.Value
	}
	return nil
}

type ProdRq struct {
	// Name of a Kafka cluster to operate on.
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic to produce to.
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// Hash of the key is used to determine the partition to produce to. By
	// default it is an empty array which is a valid key, unless key_undefined
	// is set to true and then a random partition is selected.
	KeyValue []byte `protobuf:"bytes,3,opt,name=key_value,json=keyValue,proto3" json:"key_value,omitempty"`
	// If true then the message is written to a random partition, otherwise
	// hash of key_value is used to determine the partition.
	KeyUndefined bool `protobuf:"varint,4,opt,name=key_undefined,json=keyUndefined,proto3" json:"key_undefined,omitempty"`
	// Message body.
	Message []byte `protobuf:"bytes,5,opt,name=message,proto3" json:"message,omitempty"`
	// If true then the method returns immediately after Kafka-Pixy gets the
	// produce request, and the message is written to Kafka asynchronously.
	// In that case partition and offset returned in response should be ignored.
	// If false, then a response is returned in accordance with the
	// producer.required_acks parameter, that can be one of:
	//  * no_response:    the response is returned as soon as a produce request
	//                    is delivered to a partition leader Kafka broker.
	//  * wait_for_local: the response is returned as soon as data is written
	//                    to the disk by a partition leader Kafka broker.
	//  * wait_for_all:   the response is returned after all in-sync replicas
	//                    have data committed to disk.
	AsyncMode bool `protobuf:"varint,6,opt,name=async_mode,json=asyncMode,proto3" json:"async_mode,omitempty"`
	// Headers to include with the published message
	Headers              []*RecordHeader `protobuf:"bytes,7,rep,name=headers,proto3" json:"headers,omitempty"`
	XXX_NoUnkeyedLiteral struct{}        `json:"-"`
	XXX_unrecognized     []byte          `json:"-"`
	XXX_sizecache        int32           `json:"-"`
}

func (m *ProdRq) Reset()         { *m = ProdRq{} }
func (m *ProdRq) String() string { return proto.CompactTextString(m) }
func (*ProdRq) ProtoMessage()    {}
func (*ProdRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{1}
}

func (m *ProdRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProdRq.Unmarshal(m, b)
}
func (m *ProdRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProdRq.Marshal(b, m, deterministic)
}
func (m *ProdRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProdRq.Merge(m, src)
}
func (m *ProdRq) XXX_Size() int {
	return xxx_messageInfo_ProdRq.Size(m)
}
func (m *ProdRq) XXX_DiscardUnknown() {
	xxx_messageInfo_ProdRq.DiscardUnknown(m)
}

var xxx_messageInfo_ProdRq proto.InternalMessageInfo

func (m *ProdRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *ProdRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *ProdRq) GetKeyValue() []byte {
	if m != nil {
		return m.KeyValue
	}
	return nil
}

func (m *ProdRq) GetKeyUndefined() bool {
	if m != nil {
		return m.KeyUndefined
	}
	return false
}

func (m *ProdRq) GetMessage() []byte {
	if m != nil {
		return m.Message
	}
	return nil
}

func (m *ProdRq) GetAsyncMode() bool {
	if m != nil {
		return m.AsyncMode
	}
	return false
}

func (m *ProdRq) GetHeaders() []*RecordHeader {
	if m != nil {
		return m.Headers
	}
	return nil
}

type ProdRs struct {
	// Partition the message was written to. The value only makes sense if
	// ProdReq.async_mode was false.
	Partition int32 `protobuf:"varint,1,opt,name=partition,proto3" json:"partition,omitempty"`
	// Offset the message was written to. The value only makes sense if
	// ProdReq.async_mode was false.
	Offset               int64    `protobuf:"varint,2,opt,name=offset,proto3" json:"offset,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ProdRs) Reset()         { *m = ProdRs{} }
func (m *ProdRs) String() string { return proto.CompactTextString(m) }
func (*ProdRs) ProtoMessage()    {}
func (*ProdRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{2}
}

func (m *ProdRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProdRs.Unmarshal(m, b)
}
func (m *ProdRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProdRs.Marshal(b, m, deterministic)
}
func (m *ProdRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProdRs.Merge(m, src)
}
func (m *ProdRs) XXX_Size() int {
	return xxx_messageInfo_ProdRs.Size(m)
}
func (m *ProdRs) XXX_DiscardUnknown() {
	xxx_messageInfo_ProdRs.DiscardUnknown(m)
}

var xxx_messageInfo_ProdRs proto.InternalMessageInfo

func (m *ProdRs) GetPartition() int32 {
	if m != nil {
		return m.Partition
	}
	return 0
}

func (m *ProdRs) GetOffset() int64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

type ConsNAckRq struct {
	// Name of a Kafka cluster to operate on.
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic to produce to.
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// Name of a consumer group.
	Group string `protobuf:"bytes,3,opt,name=group,proto3" json:"group,omitempty"`
	// If true then no message is acknowledged by the request.
	NoAck bool `protobuf:"varint,4,opt,name=no_ack,json=noAck,proto3" json:"no_ack,omitempty"`
	// If true and no_ack is false then the message returned by the requests is
	// automatically acknowledged by Kafka-Pixy before the request completes.
	AutoAck bool `protobuf:"varint,5,opt,name=auto_ack,json=autoAck,proto3" json:"auto_ack,omitempty"`
	// If both no_ack and auto_ack are false (by default), then ack_partition
	// and ack_offset along with cluster-group-topic determine the message that
	// should be acknowledged by the request.
	AckPartition         int32    `protobuf:"varint,6,opt,name=ack_partition,json=ackPartition,proto3" json:"ack_partition,omitempty"`
	AckOffset            int64    `protobuf:"varint,7,opt,name=ack_offset,json=ackOffset,proto3" json:"ack_offset,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ConsNAckRq) Reset()         { *m = ConsNAckRq{} }
func (m *ConsNAckRq) String() string { return proto.CompactTextString(m) }
func (*ConsNAckRq) ProtoMessage()    {}
func (*ConsNAckRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{3}
}

func (m *ConsNAckRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ConsNAckRq.Unmarshal(m, b)
}
func (m *ConsNAckRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ConsNAckRq.Marshal(b, m, deterministic)
}
func (m *ConsNAckRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ConsNAckRq.Merge(m, src)
}
func (m *ConsNAckRq) XXX_Size() int {
	return xxx_messageInfo_ConsNAckRq.Size(m)
}
func (m *ConsNAckRq) XXX_DiscardUnknown() {
	xxx_messageInfo_ConsNAckRq.DiscardUnknown(m)
}

var xxx_messageInfo_ConsNAckRq proto.InternalMessageInfo

func (m *ConsNAckRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *ConsNAckRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *ConsNAckRq) GetGroup() string {
	if m != nil {
		return m.Group
	}
	return ""
}

func (m *ConsNAckRq) GetNoAck() bool {
	if m != nil {
		return m.NoAck
	}
	return false
}

func (m *ConsNAckRq) GetAutoAck() bool {
	if m != nil {
		return m.AutoAck
	}
	return false
}

func (m *ConsNAckRq) GetAckPartition() int32 {
	if m != nil {
		return m.AckPartition
	}
	return 0
}

func (m *ConsNAckRq) GetAckOffset() int64 {
	if m != nil {
		return m.AckOffset
	}
	return 0
}

type ConsRs struct {
	// Partition the message was read from.
	Partition int32 `protobuf:"varint,1,opt,name=partition,proto3" json:"partition,omitempty"`
	// Offset of the read message in the partition.
	Offset int64 `protobuf:"varint,2,opt,name=offset,proto3" json:"offset,omitempty"`
	// Key that was used to produce the message, unless key_undefined is true,
	// then it is undefined.
	KeyValue []byte `protobuf:"bytes,3,opt,name=key_value,json=keyValue,proto3" json:"key_value,omitempty"`
	// If true then the message was produced to a random partition.
	KeyUndefined bool `protobuf:"varint,4,opt,name=key_undefined,json=keyUndefined,proto3" json:"key_undefined,omitempty"`
	// Message body
	Message []byte `protobuf:"bytes,5,opt,name=message,proto3" json:"message,omitempty"`
	// Headers associated with the message
	Headers              []*RecordHeader `protobuf:"bytes,6,rep,name=headers,proto3" json:"headers,omitempty"`
	XXX_NoUnkeyedLiteral struct{}        `json:"-"`
	XXX_unrecognized     []byte          `json:"-"`
	XXX_sizecache        int32           `json:"-"`
}

func (m *ConsRs) Reset()         { *m = ConsRs{} }
func (m *ConsRs) String() string { return proto.CompactTextString(m) }
func (*ConsRs) ProtoMessage()    {}
func (*ConsRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{4}
}

func (m *ConsRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ConsRs.Unmarshal(m, b)
}
func (m *ConsRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ConsRs.Marshal(b, m, deterministic)
}
func (m *ConsRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ConsRs.Merge(m, src)
}
func (m *ConsRs) XXX_Size() int {
	return xxx_messageInfo_ConsRs.Size(m)
}
func (m *ConsRs) XXX_DiscardUnknown() {
	xxx_messageInfo_ConsRs.DiscardUnknown(m)
}

var xxx_messageInfo_ConsRs proto.InternalMessageInfo

func (m *ConsRs) GetPartition() int32 {
	if m != nil {
		return m.Partition
	}
	return 0
}

func (m *ConsRs) GetOffset() int64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

func (m *ConsRs) GetKeyValue() []byte {
	if m != nil {
		return m.KeyValue
	}
	return nil
}

func (m *ConsRs) GetKeyUndefined() bool {
	if m != nil {
		return m.KeyUndefined
	}
	return false
}

func (m *ConsRs) GetMessage() []byte {
	if m != nil {
		return m.Message
	}
	return nil
}

func (m *ConsRs) GetHeaders() []*RecordHeader {
	if m != nil {
		return m.Headers
	}
	return nil
}

type AckRq struct {
	// Name of a Kafka cluster to operate on.
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic to produce to.
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// Name of a consumer group.
	Group string `protobuf:"bytes,3,opt,name=group,proto3" json:"group,omitempty"`
	// Partition that the acknowledged message was consumed from.
	Partition int32 `protobuf:"varint,4,opt,name=partition,proto3" json:"partition,omitempty"`
	// Offset in the partition that the acknowledged message was consumed from.
	Offset               int64    `protobuf:"varint,5,opt,name=offset,proto3" json:"offset,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AckRq) Reset()         { *m = AckRq{} }
func (m *AckRq) String() string { return proto.CompactTextString(m) }
func (*AckRq) ProtoMessage()    {}
func (*AckRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{5}
}

func (m *AckRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_AckRq.Unmarshal(m, b)
}
func (m *AckRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_AckRq.Marshal(b, m, deterministic)
}
func (m *AckRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AckRq.Merge(m, src)
}
func (m *AckRq) XXX_Size() int {
	return xxx_messageInfo_AckRq.Size(m)
}
func (m *AckRq) XXX_DiscardUnknown() {
	xxx_messageInfo_AckRq.DiscardUnknown(m)
}

var xxx_messageInfo_AckRq proto.InternalMessageInfo

func (m *AckRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *AckRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *AckRq) GetGroup() string {
	if m != nil {
		return m.Group
	}
	return ""
}

func (m *AckRq) GetPartition() int32 {
	if m != nil {
		return m.Partition
	}
	return 0
}

func (m *AckRq) GetOffset() int64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

type AckRs struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *AckRs) Reset()         { *m = AckRs{} }
func (m *AckRs) String() string { return proto.CompactTextString(m) }
func (*AckRs) ProtoMessage()    {}
func (*AckRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{6}
}

func (m *AckRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_AckRs.Unmarshal(m, b)
}
func (m *AckRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_AckRs.Marshal(b, m, deterministic)
}
func (m *AckRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AckRs.Merge(m, src)
}
func (m *AckRs) XXX_Size() int {
	return xxx_messageInfo_AckRs.Size(m)
}
func (m *AckRs) XXX_DiscardUnknown() {
	xxx_messageInfo_AckRs.DiscardUnknown(m)
}

var xxx_messageInfo_AckRs proto.InternalMessageInfo

type PartitionOffset struct {
	// The Partition this structure describes
	Partition int32 `protobuf:"varint,1,opt,name=partition,proto3" json:"partition,omitempty"`
	// The beginning offset
	Begin int64 `protobuf:"varint,2,opt,name=begin,proto3" json:"begin,omitempty"`
	// The ending offset
	End int64 `protobuf:"varint,3,opt,name=end,proto3" json:"end,omitempty"`
	// The number of messages in the partition
	Count int64 `protobuf:"varint,4,opt,name=count,proto3" json:"count,omitempty"`
	// Offset in the partition
	Offset int64 `protobuf:"varint,5,opt,name=offset,proto3" json:"offset,omitempty"`
	// The number of un-consumed messages in the partition
	Lag int64 `protobuf:"varint,6,opt,name=lag,proto3" json:"lag,omitempty"`
	// Metatdata associated with the partition
	Metadata string `protobuf:"bytes,7,opt,name=metadata,proto3" json:"metadata,omitempty"`
	// human readable representation of sparsely committed ranges
	SparseAcks           string   `protobuf:"bytes,8,opt,name=sparse_acks,json=sparseAcks,proto3" json:"sparse_acks,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *PartitionOffset) Reset()         { *m = PartitionOffset{} }
func (m *PartitionOffset) String() string { return proto.CompactTextString(m) }
func (*PartitionOffset) ProtoMessage()    {}
func (*PartitionOffset) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{7}
}

func (m *PartitionOffset) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_PartitionOffset.Unmarshal(m, b)
}
func (m *PartitionOffset) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_PartitionOffset.Marshal(b, m, deterministic)
}
func (m *PartitionOffset) XXX_Merge(src proto.Message) {
	xxx_messageInfo_PartitionOffset.Merge(m, src)
}
func (m *PartitionOffset) XXX_Size() int {
	return xxx_messageInfo_PartitionOffset.Size(m)
}
func (m *PartitionOffset) XXX_DiscardUnknown() {
	xxx_messageInfo_PartitionOffset.DiscardUnknown(m)
}

var xxx_messageInfo_PartitionOffset proto.InternalMessageInfo

func (m *PartitionOffset) GetPartition() int32 {
	if m != nil {
		return m.Partition
	}
	return 0
}

func (m *PartitionOffset) GetBegin() int64 {
	if m != nil {
		return m.Begin
	}
	return 0
}

func (m *PartitionOffset) GetEnd() int64 {
	if m != nil {
		return m.End
	}
	return 0
}

func (m *PartitionOffset) GetCount() int64 {
	if m != nil {
		return m.Count
	}
	return 0
}

func (m *PartitionOffset) GetOffset() int64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

func (m *PartitionOffset) GetLag() int64 {
	if m != nil {
		return m.Lag
	}
	return 0
}

func (m *PartitionOffset) GetMetadata() string {
	if m != nil {
		return m.Metadata
	}
	return ""
}

func (m *PartitionOffset) GetSparseAcks() string {
	if m != nil {
		return m.SparseAcks
	}
	return ""
}

type GetOffsetsRq struct {
	// Name of a Kafka cluster
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// Name of a consumer group.
	Group                string   `protobuf:"bytes,3,opt,name=group,proto3" json:"group,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *GetOffsetsRq) Reset()         { *m = GetOffsetsRq{} }
func (m *GetOffsetsRq) String() string { return proto.CompactTextString(m) }
func (*GetOffsetsRq) ProtoMessage()    {}
func (*GetOffsetsRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{8}
}

func (m *GetOffsetsRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_GetOffsetsRq.Unmarshal(m, b)
}
func (m *GetOffsetsRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_GetOffsetsRq.Marshal(b, m, deterministic)
}
func (m *GetOffsetsRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_GetOffsetsRq.Merge(m, src)
}
func (m *GetOffsetsRq) XXX_Size() int {
	return xxx_messageInfo_GetOffsetsRq.Size(m)
}
func (m *GetOffsetsRq) XXX_DiscardUnknown() {
	xxx_messageInfo_GetOffsetsRq.DiscardUnknown(m)
}

var xxx_messageInfo_GetOffsetsRq proto.InternalMessageInfo

func (m *GetOffsetsRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *GetOffsetsRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *GetOffsetsRq) GetGroup() string {
	if m != nil {
		return m.Group
	}
	return ""
}

type GetOffsetsRs struct {
	Offsets              []*PartitionOffset `protobuf:"bytes,1,rep,name=offsets,proto3" json:"offsets,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_unrecognized     []byte             `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *GetOffsetsRs) Reset()         { *m = GetOffsetsRs{} }
func (m *GetOffsetsRs) String() string { return proto.CompactTextString(m) }
func (*GetOffsetsRs) ProtoMessage()    {}
func (*GetOffsetsRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{9}
}

func (m *GetOffsetsRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_GetOffsetsRs.Unmarshal(m, b)
}
func (m *GetOffsetsRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_GetOffsetsRs.Marshal(b, m, deterministic)
}
func (m *GetOffsetsRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_GetOffsetsRs.Merge(m, src)
}
func (m *GetOffsetsRs) XXX_Size() int {
	return xxx_messageInfo_GetOffsetsRs.Size(m)
}
func (m *GetOffsetsRs) XXX_DiscardUnknown() {
	xxx_messageInfo_GetOffsetsRs.DiscardUnknown(m)
}

var xxx_messageInfo_GetOffsetsRs proto.InternalMessageInfo

func (m *GetOffsetsRs) GetOffsets() []*PartitionOffset {
	if m != nil {
		return m.Offsets
	}
	return nil
}

// Partition metadata as retrieved from kafka
type PartitionMetadata struct {
	// The Partition this structure describes
	Partition int32 `protobuf:"varint,1,opt,name=partition,proto3" json:"partition,omitempty"`
	// The node id for the kafka broker currently acting as leader for this partition.
	// If no leader exists because we are in the middle of a leader election this id will be -1.
	Leader int32 `protobuf:"varint,2,opt,name=leader,proto3" json:"leader,omitempty"`
	// The set of alive nodes that currently acts as slaves for the leader for this partition.
	Replicas []int32 `protobuf:"varint,3,rep,packed,name=replicas,proto3" json:"replicas,omitempty"`
	// The set subset of the replicas that are "caught up" to the leader
	Isr                  []int32  `protobuf:"varint,4,rep,packed,name=isr,proto3" json:"isr,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *PartitionMetadata) Reset()         { *m = PartitionMetadata{} }
func (m *PartitionMetadata) String() string { return proto.CompactTextString(m) }
func (*PartitionMetadata) ProtoMessage()    {}
func (*PartitionMetadata) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{10}
}

func (m *PartitionMetadata) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_PartitionMetadata.Unmarshal(m, b)
}
func (m *PartitionMetadata) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_PartitionMetadata.Marshal(b, m, deterministic)
}
func (m *PartitionMetadata) XXX_Merge(src proto.Message) {
	xxx_messageInfo_PartitionMetadata.Merge(m, src)
}
func (m *PartitionMetadata) XXX_Size() int {
	return xxx_messageInfo_PartitionMetadata.Size(m)
}
func (m *PartitionMetadata) XXX_DiscardUnknown() {
	xxx_messageInfo_PartitionMetadata.DiscardUnknown(m)
}

var xxx_messageInfo_PartitionMetadata proto.InternalMessageInfo

func (m *PartitionMetadata) GetPartition() int32 {
	if m != nil {
		return m.Partition
	}
	return 0
}

func (m *PartitionMetadata) GetLeader() int32 {
	if m != nil {
		return m.Leader
	}
	return 0
}

func (m *PartitionMetadata) GetReplicas() []int32 {
	if m != nil {
		return m.Replicas
	}
	return nil
}

func (m *PartitionMetadata) GetIsr() []int32 {
	if m != nil {
		return m.Isr
	}
	return nil
}

type GetTopicMetadataRq struct {
	// Name of a Kafka cluster
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// Should include partition metadata
	WithPartitions       bool     `protobuf:"varint,3,opt,name=with_partitions,json=withPartitions,proto3" json:"with_partitions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *GetTopicMetadataRq) Reset()         { *m = GetTopicMetadataRq{} }
func (m *GetTopicMetadataRq) String() string { return proto.CompactTextString(m) }
func (*GetTopicMetadataRq) ProtoMessage()    {}
func (*GetTopicMetadataRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{11}
}

func (m *GetTopicMetadataRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_GetTopicMetadataRq.Unmarshal(m, b)
}
func (m *GetTopicMetadataRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_GetTopicMetadataRq.Marshal(b, m, deterministic)
}
func (m *GetTopicMetadataRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_GetTopicMetadataRq.Merge(m, src)
}
func (m *GetTopicMetadataRq) XXX_Size() int {
	return xxx_messageInfo_GetTopicMetadataRq.Size(m)
}
func (m *GetTopicMetadataRq) XXX_DiscardUnknown() {
	xxx_messageInfo_GetTopicMetadataRq.DiscardUnknown(m)
}

var xxx_messageInfo_GetTopicMetadataRq proto.InternalMessageInfo

func (m *GetTopicMetadataRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *GetTopicMetadataRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *GetTopicMetadataRq) GetWithPartitions() bool {
	if m != nil {
		return m.WithPartitions
	}
	return false
}

type GetTopicMetadataRs struct {
	// Version of this metadata
	Version int32 `protobuf:"varint,1,opt,name=version,proto3" json:"version,omitempty"`
	// Config values
	Config map[string]string `protobuf:"bytes,2,rep,name=config,proto3" json:"config,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	// Optional list of metadata for the partitions of this topic
	Partitions           []*PartitionMetadata `protobuf:"bytes,3,rep,name=partitions,proto3" json:"partitions,omitempty"`
	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
	XXX_unrecognized     []byte               `json:"-"`
	XXX_sizecache        int32                `json:"-"`
}

func (m *GetTopicMetadataRs) Reset()         { *m = GetTopicMetadataRs{} }
func (m *GetTopicMetadataRs) String() string { return proto.CompactTextString(m) }
func (*GetTopicMetadataRs) ProtoMessage()    {}
func (*GetTopicMetadataRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{12}
}

func (m *GetTopicMetadataRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_GetTopicMetadataRs.Unmarshal(m, b)
}
func (m *GetTopicMetadataRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_GetTopicMetadataRs.Marshal(b, m, deterministic)
}
func (m *GetTopicMetadataRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_GetTopicMetadataRs.Merge(m, src)
}
func (m *GetTopicMetadataRs) XXX_Size() int {
	return xxx_messageInfo_GetTopicMetadataRs.Size(m)
}
func (m *GetTopicMetadataRs) XXX_DiscardUnknown() {
	xxx_messageInfo_GetTopicMetadataRs.DiscardUnknown(m)
}

var xxx_messageInfo_GetTopicMetadataRs proto.InternalMessageInfo

func (m *GetTopicMetadataRs) GetVersion() int32 {
	if m != nil {
		return m.Version
	}
	return 0
}

func (m *GetTopicMetadataRs) GetConfig() map[string]string {
	if m != nil {
		return m.Config
	}
	return nil
}

func (m *GetTopicMetadataRs) GetPartitions() []*PartitionMetadata {
	if m != nil {
		return m.Partitions
	}
	return nil
}

type ListTopicRs struct {
	Topics               map[string]*GetTopicMetadataRs `protobuf:"bytes,1,rep,name=topics,proto3" json:"topics,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                       `json:"-"`
	XXX_unrecognized     []byte                         `json:"-"`
	XXX_sizecache        int32                          `json:"-"`
}

func (m *ListTopicRs) Reset()         { *m = ListTopicRs{} }
func (m *ListTopicRs) String() string { return proto.CompactTextString(m) }
func (*ListTopicRs) ProtoMessage()    {}
func (*ListTopicRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{13}
}

func (m *ListTopicRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ListTopicRs.Unmarshal(m, b)
}
func (m *ListTopicRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ListTopicRs.Marshal(b, m, deterministic)
}
func (m *ListTopicRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ListTopicRs.Merge(m, src)
}
func (m *ListTopicRs) XXX_Size() int {
	return xxx_messageInfo_ListTopicRs.Size(m)
}
func (m *ListTopicRs) XXX_DiscardUnknown() {
	xxx_messageInfo_ListTopicRs.DiscardUnknown(m)
}

var xxx_messageInfo_ListTopicRs proto.InternalMessageInfo

func (m *ListTopicRs) GetTopics() map[string]*GetTopicMetadataRs {
	if m != nil {
		return m.Topics
	}
	return nil
}

type ListTopicRq struct {
	// Name of a Kafka cluster
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Should include partition metadata
	WithPartitions       bool     `protobuf:"varint,2,opt,name=with_partitions,json=withPartitions,proto3" json:"with_partitions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ListTopicRq) Reset()         { *m = ListTopicRq{} }
func (m *ListTopicRq) String() string { return proto.CompactTextString(m) }
func (*ListTopicRq) ProtoMessage()    {}
func (*ListTopicRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{14}
}

func (m *ListTopicRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ListTopicRq.Unmarshal(m, b)
}
func (m *ListTopicRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ListTopicRq.Marshal(b, m, deterministic)
}
func (m *ListTopicRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ListTopicRq.Merge(m, src)
}
func (m *ListTopicRq) XXX_Size() int {
	return xxx_messageInfo_ListTopicRq.Size(m)
}
func (m *ListTopicRq) XXX_DiscardUnknown() {
	xxx_messageInfo_ListTopicRq.DiscardUnknown(m)
}

var xxx_messageInfo_ListTopicRq proto.InternalMessageInfo

func (m *ListTopicRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *ListTopicRq) GetWithPartitions() bool {
	if m != nil {
		return m.WithPartitions
	}
	return false
}

type ListConsumersRq struct {
	// Name of a Kafka cluster
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// If non empty, return only the specified group in the result
	Group                string   `protobuf:"bytes,3,opt,name=group,proto3" json:"group,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ListConsumersRq) Reset()         { *m = ListConsumersRq{} }
func (m *ListConsumersRq) String() string { return proto.CompactTextString(m) }
func (*ListConsumersRq) ProtoMessage()    {}
func (*ListConsumersRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{15}
}

func (m *ListConsumersRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ListConsumersRq.Unmarshal(m, b)
}
func (m *ListConsumersRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ListConsumersRq.Marshal(b, m, deterministic)
}
func (m *ListConsumersRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ListConsumersRq.Merge(m, src)
}
func (m *ListConsumersRq) XXX_Size() int {
	return xxx_messageInfo_ListConsumersRq.Size(m)
}
func (m *ListConsumersRq) XXX_DiscardUnknown() {
	xxx_messageInfo_ListConsumersRq.DiscardUnknown(m)
}

var xxx_messageInfo_ListConsumersRq proto.InternalMessageInfo

func (m *ListConsumersRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *ListConsumersRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *ListConsumersRq) GetGroup() string {
	if m != nil {
		return m.Group
	}
	return ""
}

type ConsumerPartitions struct {
	Partitions           []int32  `protobuf:"varint,1,rep,packed,name=partitions,proto3" json:"partitions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ConsumerPartitions) Reset()         { *m = ConsumerPartitions{} }
func (m *ConsumerPartitions) String() string { return proto.CompactTextString(m) }
func (*ConsumerPartitions) ProtoMessage()    {}
func (*ConsumerPartitions) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{16}
}

func (m *ConsumerPartitions) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ConsumerPartitions.Unmarshal(m, b)
}
func (m *ConsumerPartitions) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ConsumerPartitions.Marshal(b, m, deterministic)
}
func (m *ConsumerPartitions) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ConsumerPartitions.Merge(m, src)
}
func (m *ConsumerPartitions) XXX_Size() int {
	return xxx_messageInfo_ConsumerPartitions.Size(m)
}
func (m *ConsumerPartitions) XXX_DiscardUnknown() {
	xxx_messageInfo_ConsumerPartitions.DiscardUnknown(m)
}

var xxx_messageInfo_ConsumerPartitions proto.InternalMessageInfo

func (m *ConsumerPartitions) GetPartitions() []int32 {
	if m != nil {
		return m.Partitions
	}
	return nil
}

type ConsumerGroups struct {
	Consumers            map[string]*ConsumerPartitions `protobuf:"bytes,1,rep,name=consumers,proto3" json:"consumers,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                       `json:"-"`
	XXX_unrecognized     []byte                         `json:"-"`
	XXX_sizecache        int32                          `json:"-"`
}

func (m *ConsumerGroups) Reset()         { *m = ConsumerGroups{} }
func (m *ConsumerGroups) String() string { return proto.CompactTextString(m) }
func (*ConsumerGroups) ProtoMessage()    {}
func (*ConsumerGroups) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{17}
}

func (m *ConsumerGroups) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ConsumerGroups.Unmarshal(m, b)
}
func (m *ConsumerGroups) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ConsumerGroups.Marshal(b, m, deterministic)
}
func (m *ConsumerGroups) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ConsumerGroups.Merge(m, src)
}
func (m *ConsumerGroups) XXX_Size() int {
	return xxx_messageInfo_ConsumerGroups.Size(m)
}
func (m *ConsumerGroups) XXX_DiscardUnknown() {
	xxx_messageInfo_ConsumerGroups.DiscardUnknown(m)
}

var xxx_messageInfo_ConsumerGroups proto.InternalMessageInfo

func (m *ConsumerGroups) GetConsumers() map[string]*ConsumerPartitions {
	if m != nil {
		return m.Consumers
	}
	return nil
}

type ListConsumersRs struct {
	Groups               map[string]*ConsumerGroups `protobuf:"bytes,1,rep,name=groups,proto3" json:"groups,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                   `json:"-"`
	XXX_unrecognized     []byte                     `json:"-"`
	XXX_sizecache        int32                      `json:"-"`
}

func (m *ListConsumersRs) Reset()         { *m = ListConsumersRs{} }
func (m *ListConsumersRs) String() string { return proto.CompactTextString(m) }
func (*ListConsumersRs) ProtoMessage()    {}
func (*ListConsumersRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{18}
}

func (m *ListConsumersRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ListConsumersRs.Unmarshal(m, b)
}
func (m *ListConsumersRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ListConsumersRs.Marshal(b, m, deterministic)
}
func (m *ListConsumersRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ListConsumersRs.Merge(m, src)
}
func (m *ListConsumersRs) XXX_Size() int {
	return xxx_messageInfo_ListConsumersRs.Size(m)
}
func (m *ListConsumersRs) XXX_DiscardUnknown() {
	xxx_messageInfo_ListConsumersRs.DiscardUnknown(m)
}

var xxx_messageInfo_ListConsumersRs proto.InternalMessageInfo

func (m *ListConsumersRs) GetGroups() map[string]*ConsumerGroups {
	if m != nil {
		return m.Groups
	}
	return nil
}

type SetOffsetsRq struct {
	// Name of a Kafka cluster
	Cluster string `protobuf:"bytes,1,opt,name=cluster,proto3" json:"cluster,omitempty"`
	// Name of a topic
	Topic string `protobuf:"bytes,2,opt,name=topic,proto3" json:"topic,omitempty"`
	// Name of a consumer group.
	Group                string             `protobuf:"bytes,3,opt,name=group,proto3" json:"group,omitempty"`
	Offsets              []*PartitionOffset `protobuf:"bytes,4,rep,name=offsets,proto3" json:"offsets,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_unrecognized     []byte             `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *SetOffsetsRq) Reset()         { *m = SetOffsetsRq{} }
func (m *SetOffsetsRq) String() string { return proto.CompactTextString(m) }
func (*SetOffsetsRq) ProtoMessage()    {}
func (*SetOffsetsRq) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{19}
}

func (m *SetOffsetsRq) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SetOffsetsRq.Unmarshal(m, b)
}
func (m *SetOffsetsRq) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SetOffsetsRq.Marshal(b, m, deterministic)
}
func (m *SetOffsetsRq) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SetOffsetsRq.Merge(m, src)
}
func (m *SetOffsetsRq) XXX_Size() int {
	return xxx_messageInfo_SetOffsetsRq.Size(m)
}
func (m *SetOffsetsRq) XXX_DiscardUnknown() {
	xxx_messageInfo_SetOffsetsRq.DiscardUnknown(m)
}

var xxx_messageInfo_SetOffsetsRq proto.InternalMessageInfo

func (m *SetOffsetsRq) GetCluster() string {
	if m != nil {
		return m.Cluster
	}
	return ""
}

func (m *SetOffsetsRq) GetTopic() string {
	if m != nil {
		return m.Topic
	}
	return ""
}

func (m *SetOffsetsRq) GetGroup() string {
	if m != nil {
		return m.Group
	}
	return ""
}

func (m *SetOffsetsRq) GetOffsets() []*PartitionOffset {
	if m != nil {
		return m.Offsets
	}
	return nil
}

type SetOffsetsRs struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SetOffsetsRs) Reset()         { *m = SetOffsetsRs{} }
func (m *SetOffsetsRs) String() string { return proto.CompactTextString(m) }
func (*SetOffsetsRs) ProtoMessage()    {}
func (*SetOffsetsRs) Descriptor() ([]byte, []int) {
	return fileDescriptor_e2eb8a121768dd12, []int{20}
}

func (m *SetOffsetsRs) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SetOffsetsRs.Unmarshal(m, b)
}
func (m *SetOffsetsRs) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SetOffsetsRs.Marshal(b, m, deterministic)
}
func (m *SetOffsetsRs) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SetOffsetsRs.Merge(m, src)
}
func (m *SetOffsetsRs) XXX_Size() int {
	return xxx_messageInfo_SetOffsetsRs.Size(m)
}
func (m *SetOffsetsRs) XXX_DiscardUnknown() {
	xxx_messageInfo_SetOffsetsRs.DiscardUnknown(m)
}

var xxx_messageInfo_SetOffsetsRs proto.InternalMessageInfo

func init() {
	proto.RegisterType((*RecordHeader)(nil), "RecordHeader")
	proto.RegisterType((*ProdRq)(nil), "ProdRq")
	proto.RegisterType((*ProdRs)(nil), "ProdRs")
	proto.RegisterType((*ConsNAckRq)(nil), "ConsNAckRq")
	proto.RegisterType((*ConsRs)(nil), "ConsRs")
	proto.RegisterType((*AckRq)(nil), "AckRq")
	proto.RegisterType((*AckRs)(nil), "AckRs")
	proto.RegisterType((*PartitionOffset)(nil), "PartitionOffset")
	proto.RegisterType((*GetOffsetsRq)(nil), "GetOffsetsRq")
	proto.RegisterType((*GetOffsetsRs)(nil), "GetOffsetsRs")
	proto.RegisterType((*PartitionMetadata)(nil), "PartitionMetadata")
	proto.RegisterType((*GetTopicMetadataRq)(nil), "GetTopicMetadataRq")
	proto.RegisterType((*GetTopicMetadataRs)(nil), "GetTopicMetadataRs")
	proto.RegisterMapType((map[string]string)(nil), "GetTopicMetadataRs.ConfigEntry")
	proto.RegisterType((*ListTopicRs)(nil), "ListTopicRs")
	proto.RegisterMapType((map[string]*GetTopicMetadataRs)(nil), "ListTopicRs.TopicsEntry")
	proto.RegisterType((*ListTopicRq)(nil), "ListTopicRq")
	proto.RegisterType((*ListConsumersRq)(nil), "ListConsumersRq")
	proto.RegisterType((*ConsumerPartitions)(nil), "ConsumerPartitions")
	proto.RegisterType((*ConsumerGroups)(nil), "ConsumerGroups")
	proto.RegisterMapType((map[string]*ConsumerPartitions)(nil), "ConsumerGroups.ConsumersEntry")
	proto.RegisterType((*ListConsumersRs)(nil), "ListConsumersRs")
	proto.RegisterMapType((map[string]*ConsumerGroups)(nil), "ListConsumersRs.GroupsEntry")
	proto.RegisterType((*SetOffsetsRq)(nil), "SetOffsetsRq")
	proto.RegisterType((*SetOffsetsRs)(nil), "SetOffsetsRs")
}

func init() { proto.RegisterFile("kafkapixy.proto", fileDescriptor_e2eb8a121768dd12) }

var fileDescriptor_e2eb8a121768dd12 = []byte{
	// 1014 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x56, 0xdb, 0x8e, 0xdb, 0x44,
	0x18, 0xde, 0x89, 0xd7, 0x76, 0xfc, 0x27, 0xbb, 0xd9, 0x0e, 0x0b, 0x18, 0xd3, 0x43, 0xe4, 0xaa,
	0x6a, 0x5a, 0x21, 0x0b, 0x2d, 0xe5, 0x54, 0xa1, 0x4a, 0xdb, 0x0a, 0x2d, 0x02, 0x5a, 0xc2, 0x6c,
	0x01, 0x89, 0x9b, 0x68, 0xd6, 0x99, 0xa4, 0x96, 0x13, 0x3b, 0xeb, 0x71, 0xda, 0xe6, 0x0e, 0x89,
	0x07, 0xe0, 0x82, 0x27, 0xe0, 0x59, 0xb8, 0xe1, 0x01, 0x10, 0x57, 0xbc, 0x00, 0x6f, 0x81, 0x66,
	0xc6, 0x87, 0x71, 0x36, 0xed, 0xa2, 0xd5, 0x72, 0xe5, 0xf9, 0x4f, 0x33, 0xdf, 0xf7, 0xfd, 0x33,
	0xe3, 0x81, 0x5e, 0x4c, 0x27, 0x31, 0x5d, 0x44, 0x2f, 0x57, 0xc1, 0x22, 0x4b, 0xf3, 0xd4, 0xff,
	0x08, 0xba, 0x84, 0x85, 0x69, 0x36, 0xfe, 0x82, 0xd1, 0x31, 0xcb, 0xf0, 0x1e, 0x18, 0x31, 0x5b,
	0xb9, 0xa8, 0x8f, 0x06, 0x0e, 0x11, 0x43, 0xbc, 0x0f, 0xe6, 0x73, 0x3a, 0x5b, 0x32, 0xb7, 0xd5,
	0x47, 0x83, 0x2e, 0x51, 0x86, 0xff, 0x37, 0x02, 0x6b, 0x98, 0xa5, 0x63, 0x72, 0x8a, 0x5d, 0xb0,
	0xc3, 0xd9, 0x92, 0xe7, 0x2c, 0x2b, 0xca, 0x4a, 0x53, 0x94, 0xe6, 0xe9, 0x22, 0x0a, 0x65, 0xa9,
	0x43, 0x94, 0x81, 0xdf, 0x05, 0x27, 0x66, 0xab, 0x91, 0x9a, 0xd4, 0x90, 0x93, 0xb6, 0x63, 0xb6,
	0xfa, 0x5e, 0xd8, 0xf8, 0x26, 0xec, 0x88, 0xe0, 0x32, 0x19, 0xb3, 0x49, 0x94, 0xb0, 0xb1, 0xbb,
	0xdd, 0x47, 0x83, 0x36, 0xe9, 0xc6, 0x6c, 0xf5, 0x5d, 0xe9, 0x13, 0x2b, 0xce, 0x19, 0xe7, 0x74,
	0xca, 0x5c, 0x53, 0xd6, 0x97, 0x26, 0xbe, 0x06, 0x40, 0xf9, 0x2a, 0x09, 0x47, 0xf3, 0x74, 0xcc,
	0x5c, 0x4b, 0xd6, 0x3a, 0xd2, 0xf3, 0x38, 0x1d, 0x33, 0x7c, 0x1b, 0xec, 0x67, 0x92, 0x27, 0x77,
	0xed, 0xbe, 0x31, 0xe8, 0x1c, 0xec, 0x04, 0x3a, 0x7b, 0x52, 0x46, 0xfd, 0x07, 0x05, 0x3b, 0x8e,
	0xaf, 0x82, 0xb3, 0xa0, 0x59, 0x1e, 0xe5, 0x51, 0x9a, 0x48, 0x7e, 0x26, 0xa9, 0x1d, 0xf8, 0x2d,
	0xb0, 0xd2, 0xc9, 0x84, 0xb3, 0x5c, 0x52, 0x34, 0x48, 0x61, 0xf9, 0x7f, 0x20, 0x80, 0x47, 0x69,
	0xc2, 0x9f, 0x1c, 0x86, 0xf1, 0x05, 0x24, 0xda, 0x07, 0x73, 0x9a, 0xa5, 0xcb, 0x85, 0x94, 0xc7,
	0x21, 0xca, 0xc0, 0x6f, 0x82, 0x95, 0xa4, 0x23, 0x1a, 0xc6, 0x85, 0x28, 0x66, 0x92, 0x1e, 0x86,
	0x31, 0x7e, 0x07, 0xda, 0x74, 0x99, 0xab, 0x80, 0x29, 0x03, 0xb6, 0xb0, 0x45, 0xe8, 0x26, 0xec,
	0xd0, 0x30, 0x1e, 0xd5, 0x04, 0x2c, 0x49, 0xa0, 0x4b, 0xc3, 0x78, 0x58, 0x71, 0x10, 0x9a, 0x85,
	0xf1, 0xa8, 0xe0, 0x61, 0x4b, 0x1e, 0x0e, 0x0d, 0xe3, 0x6f, 0x14, 0x95, 0xdf, 0x11, 0x58, 0x82,
	0xca, 0x45, 0xb5, 0xf8, 0x5f, 0xfb, 0xad, 0x35, 0xd4, 0x7a, 0x6d, 0x43, 0x7f, 0x46, 0x60, 0x5e,
	0x66, 0x2f, 0x1a, 0x52, 0x6c, 0xbf, 0x5a, 0x0a, 0xb3, 0xb1, 0x2d, 0x6c, 0x05, 0x82, 0xfb, 0x7f,
	0x22, 0xe8, 0x55, 0x1d, 0x50, 0x42, 0x9f, 0xa3, 0xee, 0x3e, 0x98, 0x27, 0x6c, 0x1a, 0x25, 0x85,
	0xb8, 0xca, 0x10, 0xc7, 0x95, 0x25, 0x63, 0x09, 0xcd, 0x20, 0x62, 0x28, 0xf2, 0xc2, 0x74, 0x99,
	0xe4, 0x12, 0x94, 0x41, 0x94, 0xf1, 0x2a, 0x40, 0xa2, 0x7e, 0x46, 0xa7, 0x72, 0x5b, 0x18, 0x44,
	0x0c, 0xb1, 0x07, 0xed, 0x39, 0xcb, 0xe9, 0x98, 0xe6, 0x54, 0xee, 0x05, 0x87, 0x54, 0x36, 0xbe,
	0x01, 0x1d, 0xbe, 0xa0, 0x19, 0x67, 0x62, 0xaf, 0x71, 0xb7, 0x2d, 0xc3, 0xa0, 0x5c, 0x87, 0x61,
	0xcc, 0xfd, 0xa7, 0xd0, 0x3d, 0x62, 0xb9, 0xe2, 0xc3, 0x2f, 0x4b, 0x6b, 0xff, 0x7e, 0x63, 0x56,
	0x8e, 0xef, 0x82, 0xad, 0xe0, 0x73, 0x17, 0xc9, 0xa6, 0xef, 0x05, 0x6b, 0x5a, 0x92, 0x32, 0xc1,
	0x7f, 0x01, 0x57, 0xaa, 0xd8, 0xe3, 0x92, 0xc7, 0xb9, 0xfb, 0x78, 0x26, 0x77, 0x8d, 0xc4, 0x66,
	0x92, 0xc2, 0x12, 0xca, 0x64, 0x6c, 0x31, 0x8b, 0x42, 0xca, 0x5d, 0xa3, 0x6f, 0x0c, 0x4c, 0x52,
	0xd9, 0x42, 0xc7, 0x88, 0x67, 0xee, 0xb6, 0x74, 0x8b, 0xa1, 0x3f, 0x07, 0x7c, 0xc4, 0xf2, 0xa7,
	0x82, 0x56, 0xb9, 0xee, 0x05, 0x04, 0xb9, 0x0d, 0xbd, 0x17, 0x51, 0xfe, 0xac, 0x3e, 0xc1, 0x5c,
	0x4a, 0xd3, 0x26, 0xbb, 0xc2, 0x5d, 0x31, 0xe3, 0xfe, 0x5f, 0x68, 0xc3, 0x7a, 0x5c, 0xac, 0xf7,
	0x9c, 0x65, 0xbc, 0xe6, 0x59, 0x9a, 0xf8, 0x63, 0xb0, 0xc2, 0x34, 0x99, 0x44, 0x53, 0xb7, 0x25,
	0x35, 0xbc, 0x11, 0x9c, 0x2d, 0x0f, 0x1e, 0xc9, 0x8c, 0xcf, 0x93, 0x3c, 0x5b, 0x91, 0x22, 0x1d,
	0x1f, 0x00, 0x34, 0xd0, 0x88, 0x62, 0x1c, 0x9c, 0x11, 0x99, 0x68, 0x59, 0xde, 0xa7, 0xd0, 0xd1,
	0xa6, 0x3a, 0xef, 0x27, 0xe3, 0x14, 0x3f, 0x99, 0xfb, 0xad, 0x4f, 0x90, 0xff, 0x0b, 0x82, 0xce,
	0xd7, 0x11, 0x57, 0xd0, 0x08, 0xc7, 0xef, 0x83, 0x25, 0xa5, 0x29, 0x7b, 0xef, 0x06, 0x5a, 0x34,
	0x90, 0x5f, 0x5e, 0x00, 0x56, 0x79, 0xde, 0x13, 0xe8, 0x68, 0xee, 0x0d, 0x8b, 0xdf, 0xd1, 0x17,
	0xef, 0x1c, 0xbc, 0xb1, 0x41, 0x09, 0x1d, 0xd1, 0x50, 0x07, 0xf4, 0xba, 0x96, 0x6e, 0x68, 0x5e,
	0x6b, 0x63, 0xf3, 0x7e, 0x80, 0x9e, 0x98, 0x51, 0xdc, 0xb2, 0xcb, 0x39, 0xcb, 0x2e, 0xef, 0xe4,
	0xdc, 0x03, 0x5c, 0x4e, 0x5a, 0x2f, 0x87, 0xaf, 0x37, 0x3a, 0x88, 0xe4, 0x9e, 0xd5, 0x3c, 0xfe,
	0x6f, 0x08, 0x76, 0xcb, 0xb2, 0x23, 0x31, 0x0f, 0xc7, 0x9f, 0x81, 0x13, 0x96, 0xe8, 0x0a, 0xe1,
	0xaf, 0x07, 0xcd, 0x9c, 0xca, 0x2c, 0xe4, 0xaf, 0x0b, 0xbc, 0x6f, 0xeb, 0xf9, 0xfe, 0x7b, 0x13,
	0xce, 0x02, 0xd7, 0x9b, 0xf0, 0x2b, 0x5a, 0xd7, 0x8c, 0xe3, 0x7b, 0x60, 0x49, 0xda, 0x25, 0xc2,
	0xab, 0xc1, 0x5a, 0x46, 0xa0, 0x90, 0x16, 0xdb, 0x43, 0xe5, 0x7a, 0x5f, 0x42, 0x47, 0x73, 0x6f,
	0x40, 0x76, 0xab, 0x89, 0xac, 0xb7, 0xc6, 0x5b, 0x47, 0xf5, 0x13, 0x82, 0xee, 0xf1, 0xa5, 0x5f,
	0x80, 0xfa, 0x85, 0xb7, 0x7d, 0xde, 0x85, 0xb7, 0xdb, 0x40, 0xc0, 0x0f, 0xfe, 0x69, 0x81, 0xf3,
	0x95, 0x78, 0xf4, 0x0d, 0xa3, 0x97, 0x2b, 0x7c, 0x0d, 0x6c, 0xf1, 0xae, 0x59, 0x86, 0x0c, 0xdb,
	0x81, 0x7a, 0xbf, 0x79, 0xc5, 0x80, 0xfb, 0x5b, 0xf8, 0x96, 0x3c, 0xa7, 0x82, 0x9c, 0x78, 0xb8,
	0xe0, 0x4e, 0x50, 0xbf, 0x61, 0x3c, 0x3b, 0x50, 0xaf, 0x00, 0x7f, 0x0b, 0xbf, 0x0d, 0x86, 0x08,
	0x5b, 0x81, 0x8a, 0xa8, 0xaf, 0x08, 0xbc, 0x07, 0x50, 0xdf, 0xd4, 0x78, 0x27, 0xd0, 0x7f, 0x06,
	0x5e, 0xc3, 0x2c, 0xb2, 0x8f, 0xf5, 0xec, 0xe3, 0x66, 0xf6, 0x71, 0x33, 0xfb, 0x2e, 0x40, 0x75,
	0xec, 0x38, 0xee, 0x6a, 0xc7, 0xfe, 0xd4, 0xd3, 0x2d, 0x91, 0xfb, 0x21, 0xec, 0x34, 0x5a, 0x8f,
	0xf7, 0xd6, 0xb6, 0xc2, 0xa9, 0xb7, 0xee, 0x11, 0x65, 0x0f, 0x60, 0x6f, 0xfd, 0xe8, 0xe3, 0x0d,
	0xb7, 0xc1, 0xa9, 0xb7, 0xe9, 0x8a, 0xf0, 0xb7, 0x1e, 0xde, 0x81, 0x2b, 0x73, 0x1a, 0xcd, 0xa6,
	0xcb, 0x24, 0xa8, 0xde, 0xd9, 0x0f, 0x77, 0x2b, 0xf5, 0x87, 0xe2, 0xc5, 0x3d, 0x44, 0x3f, 0xb6,
	0x16, 0x27, 0x27, 0x96, 0x7c, 0x7e, 0x7f, 0xf0, 0x6f, 0x00, 0x00, 0x00, 0xff, 0xff, 0x6d, 0xd4,
	0x27, 0x85, 0x91, 0x0b, 0x00, 0x00,
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// KafkaPixyClient is the client API for KafkaPixy service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type KafkaPixyClient interface {
	// Produce writes a message to a Kafka topic.
	//
	// If ProdReq.async_mode is false (default value) then the request will
	// block until the message is written to all ISR. In this case the respose
	// will contain the partition and offset of the message. This has to be
	// used to achive at-least-once deliverability guarantee.
	// If ProdReq.async_mode is true, then Kafka-Pixy returns immediately after
	// it gets the request and performs write on the backgroud. This mode
	// ensures highest throughput but messages can be lost, e.g. if the host
	// crashes before Kafka-Pixy has a chance to complete write.
	//
	// Hash of ProdReq.key_value is used to determine a partition that the
	// message should be written to. If you want a message to go to an random
	// partition then set ProdReq.key_undefined to true. Note that if both
	// ProdReq.key_undefined and ProdReq.key_value are left default, which is
	// empty string and false respectively, then messages will be consitently
	// written to a partiticular partition selected by the hash of an empty
	// string.
	//
	// gRPC error codes:
	//  * Invalid Argument (3): see the status description for details;
	//  * Internal (13): see the status description and logs for details;
	//  * Unavailable (14): the service is shutting down.
	Produce(ctx context.Context, in *ProdRq, opts ...grpc.CallOption) (*ProdRs, error)
	// Consume reads a message from a topic and optionally acknowledges a
	// message previously consumed from the same topic.
	//
	// Requests are performed in long polling fation, that is if all available
	// messages have been consumed then the request will block for
	// config.yaml:proxies.<cluster>.consumer.long_polling_timeout waiting for
	// new messages. If no new messages is produced while waiting the request
	// will return gRPC error with 408 status code.
	//
	// To consume the first message set ConsNAckReq.no_ack to true, since there
	// is no message to acknowledge at this point. In the second and all
	// subsequent calls of the method set ConsNAckReq.ack_partition and
	// ConsNAckReq.ack_offset to the respective values of ConsRes returned by
	// the previous method call. To acknowledge the last consumed message before
	// teminating the application call Ack method.
	//
	// If a message is not acknowledged within
	// config.yaml:proxies.<cluster>.consumer.ack_timeout the it will be returned
	// by Kafka-Pixy in ConsRes again possibly to another application.
	//
	// If at-least-once delivery guarantee and retries are not desirable, then
	// you can set ConsNAckReq.auto_ack to true and Kafka-Pixy will acknowledge
	// messages automatically before returning them in ConsRes.
	//
	// gRPC error codes:
	//  * Not Found (5): It just means that all message has been consumed and
	//    the long polling timeout has elaspsed. Just keep calling this method
	//    in a loop;
	//  * Resource Exhausted (8): too many consume requests. Either reduce the
	//    number of consuming threads or increase
	//    config.yaml:proxies.<cluster>.consumer.channel_buffer_size;
	//  * Invalid Argument (3): see the status description for details;
	//  * Internal (13): see the status description and logs for details;
	//  * Unavailable (14): the service is shutting down.
	ConsumeNAck(ctx context.Context, in *ConsNAckRq, opts ...grpc.CallOption) (*ConsRs, error)
	// Ack acknowledges a message earlier consumed from a topic.
	//
	// This method is provided solely to acknowledge the last consumed message
	// before the application terminates. In all other cases ConsumeNAck should
	// be used.
	//
	// gRPC error codes:
	//  * Invalid Argument (3): see the status description for details;
	//  * Internal (13): see the status description and logs for details;
	Ack(ctx context.Context, in *AckRq, opts ...grpc.CallOption) (*AckRs, error)
	// Fetches partition offsets for the specified topic and group
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on offset request
	//  * NotFound (5): If the group and or topic does not exist
	GetOffsets(ctx context.Context, in *GetOffsetsRq, opts ...grpc.CallOption) (*GetOffsetsRs, error)
	// Sets partition offsets for the specified topic and group.
	// NOTE: Although the request accepts the PartitionOffset object i
	// only 'Partition', 'Offset' and 'Metadata' are set by this method
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on offset request
	//  * NotFound (5): If the group and or topic does not exist
	SetOffsets(ctx context.Context, in *SetOffsetsRq, opts ...grpc.CallOption) (*SetOffsetsRs, error)
	// Lists all topics and metadata with optional metadata for the partitions of the topic
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on request
	ListTopics(ctx context.Context, in *ListTopicRq, opts ...grpc.CallOption) (*ListTopicRs, error)
	// Lists all consumers of a topic
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on request
	ListConsumers(ctx context.Context, in *ListConsumersRq, opts ...grpc.CallOption) (*ListConsumersRs, error)
	// Fetches topic metadata and optional metadata for the partitions of the topic
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on request
	//  * NotFound (5): If the topic does not exist
	GetTopicMetadata(ctx context.Context, in *GetTopicMetadataRq, opts ...grpc.CallOption) (*GetTopicMetadataRs, error)
}

type kafkaPixyClient struct {
	cc *grpc.ClientConn
}

func NewKafkaPixyClient(cc *grpc.ClientConn) KafkaPixyClient {
	return &kafkaPixyClient{cc}
}

func (c *kafkaPixyClient) Produce(ctx context.Context, in *ProdRq, opts ...grpc.CallOption) (*ProdRs, error) {
	out := new(ProdRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/Produce", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) ConsumeNAck(ctx context.Context, in *ConsNAckRq, opts ...grpc.CallOption) (*ConsRs, error) {
	out := new(ConsRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/ConsumeNAck", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) Ack(ctx context.Context, in *AckRq, opts ...grpc.CallOption) (*AckRs, error) {
	out := new(AckRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/Ack", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) GetOffsets(ctx context.Context, in *GetOffsetsRq, opts ...grpc.CallOption) (*GetOffsetsRs, error) {
	out := new(GetOffsetsRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/GetOffsets", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) SetOffsets(ctx context.Context, in *SetOffsetsRq, opts ...grpc.CallOption) (*SetOffsetsRs, error) {
	out := new(SetOffsetsRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/SetOffsets", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) ListTopics(ctx context.Context, in *ListTopicRq, opts ...grpc.CallOption) (*ListTopicRs, error) {
	out := new(ListTopicRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/ListTopics", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) ListConsumers(ctx context.Context, in *ListConsumersRq, opts ...grpc.CallOption) (*ListConsumersRs, error) {
	out := new(ListConsumersRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/ListConsumers", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *kafkaPixyClient) GetTopicMetadata(ctx context.Context, in *GetTopicMetadataRq, opts ...grpc.CallOption) (*GetTopicMetadataRs, error) {
	out := new(GetTopicMetadataRs)
	err := c.cc.Invoke(ctx, "/KafkaPixy/GetTopicMetadata", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// KafkaPixyServer is the server API for KafkaPixy service.
type KafkaPixyServer interface {
	// Produce writes a message to a Kafka topic.
	//
	// If ProdReq.async_mode is false (default value) then the request will
	// block until the message is written to all ISR. In this case the respose
	// will contain the partition and offset of the message. This has to be
	// used to achive at-least-once deliverability guarantee.
	// If ProdReq.async_mode is true, then Kafka-Pixy returns immediately after
	// it gets the request and performs write on the backgroud. This mode
	// ensures highest throughput but messages can be lost, e.g. if the host
	// crashes before Kafka-Pixy has a chance to complete write.
	//
	// Hash of ProdReq.key_value is used to determine a partition that the
	// message should be written to. If you want a message to go to an random
	// partition then set ProdReq.key_undefined to true. Note that if both
	// ProdReq.key_undefined and ProdReq.key_value are left default, which is
	// empty string and false respectively, then messages will be consitently
	// written to a partiticular partition selected by the hash of an empty
	// string.
	//
	// gRPC error codes:
	//  * Invalid Argument (3): see the status description for details;
	//  * Internal (13): see the status description and logs for details;
	//  * Unavailable (14): the service is shutting down.
	Produce(context.Context, *ProdRq) (*ProdRs, error)
	// Consume reads a message from a topic and optionally acknowledges a
	// message previously consumed from the same topic.
	//
	// Requests are performed in long polling fation, that is if all available
	// messages have been consumed then the request will block for
	// config.yaml:proxies.<cluster>.consumer.long_polling_timeout waiting for
	// new messages. If no new messages is produced while waiting the request
	// will return gRPC error with 408 status code.
	//
	// To consume the first message set ConsNAckReq.no_ack to true, since there
	// is no message to acknowledge at this point. In the second and all
	// subsequent calls of the method set ConsNAckReq.ack_partition and
	// ConsNAckReq.ack_offset to the respective values of ConsRes returned by
	// the previous method call. To acknowledge the last consumed message before
	// teminating the application call Ack method.
	//
	// If a message is not acknowledged within
	// config.yaml:proxies.<cluster>.consumer.ack_timeout the it will be returned
	// by Kafka-Pixy in ConsRes again possibly to another application.
	//
	// If at-least-once delivery guarantee and retries are not desirable, then
	// you can set ConsNAckReq.auto_ack to true and Kafka-Pixy will acknowledge
	// messages automatically before returning them in ConsRes.
	//
	// gRPC error codes:
	//  * Not Found (5): It just means that all message has been consumed and
	//    the long polling timeout has elaspsed. Just keep calling this method
	//    in a loop;
	//  * Resource Exhausted (8): too many consume requests. Either reduce the
	//    number of consuming threads or increase
	//    config.yaml:proxies.<cluster>.consumer.channel_buffer_size;
	//  * Invalid Argument (3): see the status description for details;
	//  * Internal (13): see the status description and logs for details;
	//  * Unavailable (14): the service is shutting down.
	ConsumeNAck(context.Context, *ConsNAckRq) (*ConsRs, error)
	// Ack acknowledges a message earlier consumed from a topic.
	//
	// This method is provided solely to acknowledge the last consumed message
	// before the application terminates. In all other cases ConsumeNAck should
	// be used.
	//
	// gRPC error codes:
	//  * Invalid Argument (3): see the status description for details;
	//  * Internal (13): see the status description and logs for details;
	Ack(context.Context, *AckRq) (*AckRs, error)
	// Fetches partition offsets for the specified topic and group
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on offset request
	//  * NotFound (5): If the group and or topic does not exist
	GetOffsets(context.Context, *GetOffsetsRq) (*GetOffsetsRs, error)
	// Sets partition offsets for the specified topic and group.
	// NOTE: Although the request accepts the PartitionOffset object i
	// only 'Partition', 'Offset' and 'Metadata' are set by this method
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on offset request
	//  * NotFound (5): If the group and or topic does not exist
	SetOffsets(context.Context, *SetOffsetsRq) (*SetOffsetsRs, error)
	// Lists all topics and metadata with optional metadata for the partitions of the topic
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on request
	ListTopics(context.Context, *ListTopicRq) (*ListTopicRs, error)
	// Lists all consumers of a topic
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on request
	ListConsumers(context.Context, *ListConsumersRq) (*ListConsumersRs, error)
	// Fetches topic metadata and optional metadata for the partitions of the topic
	//
	// gRPC error codes:
	//  * Invalid Argument (3): If unable to find the cluster named in the request
	//  * Internal (13): If Kafka returns an error on request
	//  * NotFound (5): If the topic does not exist
	GetTopicMetadata(context.Context, *GetTopicMetadataRq) (*GetTopicMetadataRs, error)
}

// UnimplementedKafkaPixyServer can be embedded to have forward compatible implementations.
type UnimplementedKafkaPixyServer struct {
}

func (*UnimplementedKafkaPixyServer) Produce(ctx context.Context, req *ProdRq) (*ProdRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Produce not implemented")
}
func (*UnimplementedKafkaPixyServer) ConsumeNAck(ctx context.Context, req *ConsNAckRq) (*ConsRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ConsumeNAck not implemented")
}
func (*UnimplementedKafkaPixyServer) Ack(ctx context.Context, req *AckRq) (*AckRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Ack not implemented")
}
func (*UnimplementedKafkaPixyServer) GetOffsets(ctx context.Context, req *GetOffsetsRq) (*GetOffsetsRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method GetOffsets not implemented")
}
func (*UnimplementedKafkaPixyServer) SetOffsets(ctx context.Context, req *SetOffsetsRq) (*SetOffsetsRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method SetOffsets not implemented")
}
func (*UnimplementedKafkaPixyServer) ListTopics(ctx context.Context, req *ListTopicRq) (*ListTopicRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ListTopics not implemented")
}
func (*UnimplementedKafkaPixyServer) ListConsumers(ctx context.Context, req *ListConsumersRq) (*ListConsumersRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ListConsumers not implemented")
}
func (*UnimplementedKafkaPixyServer) GetTopicMetadata(ctx context.Context, req *GetTopicMetadataRq) (*GetTopicMetadataRs, error) {
	return nil, status.Errorf(codes.Unimplemented, "method GetTopicMetadata not implemented")
}

func RegisterKafkaPixyServer(s *grpc.Server, srv KafkaPixyServer) {
	s.RegisterService(&_KafkaPixy_serviceDesc, srv)
}

func _KafkaPixy_Produce_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ProdRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).Produce(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/Produce",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).Produce(ctx, req.(*ProdRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_ConsumeNAck_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ConsNAckRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).ConsumeNAck(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/ConsumeNAck",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).ConsumeNAck(ctx, req.(*ConsNAckRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_Ack_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(AckRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).Ack(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/Ack",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).Ack(ctx, req.(*AckRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_GetOffsets_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(GetOffsetsRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).GetOffsets(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/GetOffsets",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).GetOffsets(ctx, req.(*GetOffsetsRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_SetOffsets_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SetOffsetsRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).SetOffsets(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/SetOffsets",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).SetOffsets(ctx, req.(*SetOffsetsRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_ListTopics_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ListTopicRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).ListTopics(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/ListTopics",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).ListTopics(ctx, req.(*ListTopicRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_ListConsumers_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ListConsumersRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).ListConsumers(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/ListConsumers",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).ListConsumers(ctx, req.(*ListConsumersRq))
	}
	return interceptor(ctx, in, info, handler)
}

func _KafkaPixy_GetTopicMetadata_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(GetTopicMetadataRq)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(KafkaPixyServer).GetTopicMetadata(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/KafkaPixy/GetTopicMetadata",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(KafkaPixyServer).GetTopicMetadata(ctx, req.(*GetTopicMetadataRq))
	}
	return interceptor(ctx, in, info, handler)
}

var _KafkaPixy_serviceDesc = grpc.ServiceDesc{
	ServiceName: "KafkaPixy",
	HandlerType: (*KafkaPixyServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Produce",
			Handler:    _KafkaPixy_Produce_Handler,
		},
		{
			MethodName: "ConsumeNAck",
			Handler:    _KafkaPixy_ConsumeNAck_Handler,
		},
		{
			MethodName: "Ack",
			Handler:    _KafkaPixy_Ack_Handler,
		},
		{
			MethodName: "GetOffsets",
			Handler:    _KafkaPixy_GetOffsets_Handler,
		},
		{
			MethodName: "SetOffsets",
			Handler:    _KafkaPixy_SetOffsets_Handler,
		},
		{
			MethodName: "ListTopics",
			Handler:    _KafkaPixy_ListTopics_Handler,
		},
		{
			MethodName: "ListConsumers",
			Handler:    _KafkaPixy_ListConsumers_Handler,
		},
		{
			MethodName: "GetTopicMetadata",
			Handler:    _KafkaPixy_GetTopicMetadata_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "kafkapixy.proto",
}
